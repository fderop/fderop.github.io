---
layout: classic
title: "The Upcoming Market for Structured Biological Data"
author: "@fvderop"
date: 2024-11-20
pdf: /assets/pdfs/market_for_data.pdf
---

The best way to evaluate the quality of any information is to use that information to make a prediction. Inversely, if a model consistently makes accurate predictions, it can be assumed that it is based on high-quality information. Drugs and therapies are biology's most coveted predictions. After all, a drug is an intervention on an ill person's future - "if we administer you this drug, you will get better". Disease and death thus generate a large demand for accurate biological predictions, which can only be realised with quality information. Under normal circumstances, markets will attempt to satisfy large aggregate demands by increasing supply. This happens by re-allocating resources away from less efficient processes, and increasing production efficiency through technological advance.

Despite large amounts of resources allocated, and broad technological progress across the industry, the rate of new drug discoveries has not increased. In fact, the efficiency of drug approval has *decreased* both in time and per unit of money spent, with cost per drug doubling approximately every nine years. This phenomenon has been dubbed ["Eroom's law"](https://www.nature.com/articles/nrd3681), an antithetical reference to Moore's law. When markets do not operate efficiently, we should examine how free they are. As expected, the pharmaceutical and biotechnology industries operate under heavy regulation. For example, the mean duration of discovery and pre-clinical research is [around 5 years](https://www.frontiersin.org/journals/drug-discovery/articles/10.3389/fddsv.2023.1201419/full), while ensuing clinical trials take [approximately 7 years](https://www.nature.com/articles/nrd.2017.21) and the regulatory approval procedure consumes another year. Failure to proceed to the clinical stage is also commonplace. The estimated industry-wide cost of failed oncology trials is [$50â€“$60 billion annually](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2807710). Exact calculations are difficult, but one study estimates that [only around 7%](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2820562) of the total cost of approved drugs is spent on non-clinical research! Efficiency is also lost to approval-seeking rather than truth-seeking: FDA approval becomes the ultimate goal, while patient benefit becomes an afterthought.

Can regulatory pressure be the main driver in reducing efficiency? Let us compare pharma/biotech with another highly-regulated industry: finance. Both industries attempt to make predictions on complex and stochastic systems. How is finance approaching this problem? Real estate in Mahwah, New Jersey commands a premium due to proximity to the New York Stock Exchange's data center, giving traders a latency - and thus data/execution - advantage. Bloomberg licenses its terminal for $25,000 per year. Companies like Experian and Equifax sell consumer credit data. The financial industry is the largest private consumer of satellite data, which is used to examine weather trends, crop growth, migration, energy consumption. These are all very structured datasets, and usually large in size. High quality data is thus a commodity that is valued by actors who wish to make accurate predictions. Moreover, it appears that this strategy is highly efficient, since algorithmic traders have [outpaced](https://paperswithbacktest.com/wiki/comparison-of-value-and-growth-stocks) value investing.

If the same developments in algorithms and models are available to everyone, and incentives are just as great, then why has a data-driven strategy not penetrated as deeply as in biotech and pharma as it has in finance? We could theorize that biological models do not scale as well with data and compute as financial or language models have. Perhaps the matter is just that much more complicated. However, machine learning models have recently begun outperforming humans in a variety of medical tasks: LLMs have outperformed physicians at [some diagnostic tasks](https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1380148/full), and computer vision models have outperformed pathologists at [interpreting medical imaging](https://pubmed.ncbi.nlm.nih.gov/38503537/). More relevant to R&D, the field of bioinformatics has produced a number of predictive tools. Alphafold allows for rapid [prediction of protein structure](https://www.nature.com/articles/s41586-021-03819-2) and even [molecular interactions](https://www.nature.com/articles/s41586-024-07487-w) based just on amino-acid sequence. High-dimensional sequencing data has been used to predict the [relationship between genomes and gene expression](https://www.nature.com/articles/s41586-021-04262-z). We must seek the answer to our question in the type of data used, and perhaps more importantly, how it is generated. Whereas finance ingests structured data, biotech and pharma rely mostly on iterative experimental campaigns: small rounds of experiments are used to generate different data types, such as imaging, biochemical assays, and in recent decades, various -omics. These datasets are combined and interpreted by skilled and experienced professionals. If high-dimensional data is presented, it is usually first processed and dimension-reduced until palatable to humans. A new hypothesis (prediction) is proposed and tested, and this process iteratively continues until a hypothetical drug is found to be effective. Training a model on a large number of heterogeneous datasets is challenging. In addition, this step-wise approach demands flexibility of execution, which is why human operators are preferred, which injects a number of confounding effects into the data: differences between operators or even the same operator at different times, time lag between experiments, biased reporting, ... - each of which multiplicatively increases noise in the data. Experienced professionals are deployed to fill data gaps with intuition - which is why they are so highly valued - but this strategy does not scale.

We can observe that high levels of noise, custom reagents, and the challenge of describing protocols translate to [low reproducibility](https://elifesciences.org/articles/75830) in academic research. Even in bioinformatics, an [extreme minority](https://academic.oup.com/bib/article/24/6/bbad375/7326135) of results can be reproduced from published code and data. Given that academic talent flows towards biotech, we can expect that the industry has inherited some of these flaws. Lab automation could theoretically provide relief and help increase robustness, but small experiment scales make it difficult to justify the cost and lead time associated with it. Paradoxically, the small size of experiments is itself a consequence of widespread uncertainty. As a result, most lab automation capabilities are currently used for either manufacturing, diagnostics, or variants of high-throughput hit screening. We could consider doubling down and simply becoming better at automating low-n work, but I think this path is sub-optimal in the long run. Wide variety in protocols and techniques complicates low-n automation, which is why generalists such as [Emerald Cloud Labs](https://www.emeraldcloudlab.com/) and Strateos have failed. I expect that specialists who focus on one single assay or service, such as [Plasmidsaurus](https://plasmidsaurus.com/) and [Adaptyv Bio](https://www.adaptyvbio.com/), will perform much better. Once these platforms reach critical mass, they will be able to benefit from economies of scale and have room to finetune their quality. Furthermore, they will be able to scale their services horizontally to fill large batch orders. Scale has been able to pull this off in a different industry, providing large datasets for generative AI, automotive industries and government. The demand for large, well-curated, structured datasets in industries where ML models have penetrated sufficiently deep is vast.

Silicon and energy have been commodified, but biological training data has not. The biotech industry should have the confidence to generate large (in the order of $10M) structured, high-dimensional datasets across a large number of technical and biological replicates with the explicit intent to feed this data into machine learning models. Such large datasets could even be "rented" out to customers who train models on them without ever seeing the data by using homomorphic encryption. When it is proven that this mode of data collection will lead to better models, and predictive power grows, this will also attract talent from other industries. Workers prefer to spend their time efficiently, and noisy data and inefficient collection act as a negative filter on this natural pressure. Remove this filter, and the system will self-reinforce. This evolution has already been set in motion, and I am hopeful that we will see a new generation of model-derived medicine and therapies enter clinical trials before the end of the decade.

### References
* Scannell et al. (2012). Diagnosing the decline in pharmaceutical R&D efficiency. [*Nature Reviews Drug Discovery*](https://www.nature.com/articles/nrd3681)
* Singh et al. (2023). Drug discovery and development: introduction to the general public and patient groups. [*Frontiers in Drug Discovery*](https://www.frontiersin.org/journals/drug-discovery/articles/10.3389/fddsv.2023.1201419/full)
* Martin et al. (2017). Clinical trial cycle times continue to increase despite industry efforts. [*Nature Reviews Drug Discovery*](https://www.nature.com/articles/nrd.2017.21)
* Jentzsch et al. (2023). Costs and Causes of Oncology Drug Attrition With the Example of Insulin-Like Growth Factor-1 Receptor Inhibitors. [*JAMA Network Open*](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2807710)
* Sertkaya et al. (2024). Costs of Drug Development and Research and Development Intensity in the US, 2000-2018. [*JAMA Network Open*](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2820562)
* Jumper et al. (2021). Highly accurate protein structure prediction with AlphaFold. [*Nature*](https://www.nature.com/articles/s41586-021-03819-2)
* Abramson et al. (2024). Scalable prediction of protein-protein interactions using AlphaFold. [*Nature*](https://www.nature.com/articles/s41586-024-07487-w)
* RÃ­os-Hoyo et al. (2024). Evaluation of large language models as a diagnostic aid for complex medical cases. [*Frontiers in Medicine*](https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1380148/full)
* Bhave et al. (2024). Deep learning to detect left ventricular structural abnormalities in chest X-rays. [*European Heart Journal*](https://pubmed.ncbi.nlm.nih.gov/38503537/)
* Janssens et al. (2022). Decoding gene regulation in the fly brain. [*Nature*](https://www.nature.com/articles/s41586-021-04262-z)
* Rodgers & Collings (2021). Reproducibility in Cancer Biology: What have we learned? [*eLife*](https://elifesciences.org/articles/75830)
* Ziemann et al. (2023). The five pillars of computational reproducibility: bioinformatics and beyond. [*Briefings in Bioinformatics*](https://academic.oup.com/bib/article/24/6/bbad375/7326135)
